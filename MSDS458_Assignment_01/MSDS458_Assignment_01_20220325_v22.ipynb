{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orJYAIqHkn5y"
   },
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/NorthwesternHeader.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy62HRWykn53"
   },
   "source": [
    "## MSDS458 Research Assignment 1:\n",
    "\n",
    "* In this notebook, we will build a `DNN` model for classifying MNIST digits. The `DNN` model will consist of 784 input nodes, a hidden layer with 128 nodes and 10 output nodes (corresponding to the 10 digits). \n",
    "* We use `mnist.load_data()` to get the 70,000 images divided into a set of 60,000 training images and 10,000 test images. We hold back 5,000 of the 60,000 training images for validation. \n",
    "* After training and evaluating our `DNN` model we analyze its performance. In particular, we use confusion matrices to compare the `predicted classes` with the `class labels` to try to determine why some images were misclassified by the model.\n",
    "* We then obtain the 60,000 `activation values` of one of the hidden nodes for the (original) set of training data. We want to use these `activation values` as \"proxies\" for the predicted classes of the 60,000 images. \n",
    "* And just like we compared the `predicted classes` with the `class labels` using confusion matrices to determine the efficacy of the model, we use `box plots` to visualize the relationship between the `activation values` of one hidden node and the `class labels`. We don't expect these activation values to have much \"predictive power\". In fact, the same activation values can be associated with multiple class labels resulting in a lot of overlap in the `box plots`.\n",
    "* We also perform similar experiments comparing the values at two pixel locations in the images with the class labels. This time we use `scatter plots` to visualize the relationship between the pair of pixel values with the class labels (represented by different colored dots).\n",
    "* Pixel values at two locations in image should not have much predictive value. To improve on this approach, we the PCA decomposition on both the raw data of 784 pixel values and 128 hidden node activation values to reduce the number of features to 2 in each case. Once again, we use a `scatter plot` to visualize the correlation between the two principal component values and the class labels.\n",
    "*  Finally, we use a Random Forest Classifier to find the relative importance of the 784 features (pixels) in the training set. We then select the 70 most important feature (pixels) from the training, validation and test images to test our 'best' model on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dTs_msbkn54"
   },
   "source": [
    "## Importing Packages \n",
    "\n",
    "* First we import all the packages that will be used in the assignment.\n",
    "\n",
    "* Since Keras is integrated in TensorFlow 2.x, we import `keras` from `tensorflow` and use `tenserflow.keras.xxx` to import all other Keras packages. The seed argument produces a deterministic sequence of tensors across multiple calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RE2Olj5kn54"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from packaging import version\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl  # EA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "#from plot_keras_history import plot_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZU6vom71kn55"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XwlONPLkn55"
   },
   "source": [
    "## Verify TensorFlow version and Keras version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HSeZcnpkn55",
    "outputId": "e820161f-8362-4466-9a9a-dda2d84a0c4e"
   },
   "outputs": [],
   "source": [
    "print(\"This notebook requires TensorFlow 2.0 or above\")\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJ9PpKc0kn56",
    "outputId": "efa82e49-730d-405e-d7c3-229f1ed2111a"
   },
   "outputs": [],
   "source": [
    "print(\"Keras version: \", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr8DPDQ2kn56"
   },
   "source": [
    "## Mount Google Drive to Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEU8D6Jekn56"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_validation_report(test_labels, predictions):\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(test_labels, predictions))\n",
    "    print('Accuracy Score: {}'.format(accuracy_score(test_labels, predictions)))\n",
    "    print('Root Mean Square Error: {}'.format(np.sqrt(MSE(test_labels, predictions)))) \n",
    "    \n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.75,  cbar=False, ax=ax,cmap='Blues',linecolor='white')\n",
    "    #  square=True,\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAWHh09ckn56"
   },
   "source": [
    "## Loading MNIST Dataset\n",
    "\n",
    "* The MNIST dataset of handwritten digits has a training set of 60,000 images, and a test set of 10,000 images. It comes prepackaged as part of `tf.Keras`. Use the `tf.keras.datasets.mnist.load_data` to the get these datasets (and the corresponding labels) as Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b-9FrSukn57",
    "outputId": "4b07da07-f9bf-4b94-b151-2aceac476727"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test)= tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qwDLcPrkn57"
   },
   "source": [
    "* Tuples of Numpy arrays: `(x_train, y_train)`, `(x_test, y_test)`\n",
    "* `x_train`, `x_test`: uint8 arrays of grayscale image data with shapes (num_samples, 28, 28).\n",
    "* `y_train`, `y_test`: uint8 arrays of digit labels (integers in range 0-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uG2UuvWmkn57"
   },
   "source": [
    "## EDA Training and Test Sets\n",
    "\n",
    "* Inspect the training and test sets as well as their labels as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-3GtF0okn57",
    "outputId": "c387080e-fafb-4ce9-d44b-c2cec990d611"
   },
   "outputs": [],
   "source": [
    "print('x_train:\\t{}'.format(x_train.shape))\n",
    "print('y_train:\\t{}'.format(y_train.shape))\n",
    "print('x_test:\\t\\t{}'.format(x_test.shape))\n",
    "print('y_test:\\t\\t{}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aknb-WH8kn57"
   },
   "source": [
    "## Review labels for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KLr92thkn58",
    "outputId": "6c404b30-a43e-4c7c-e69f-c695b61437d2"
   },
   "outputs": [],
   "source": [
    "print(\"First ten labels training dataset:\\n {}\\n\".format(y_train[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qR5_IJZdkn58"
   },
   "source": [
    "## Find frequency of each label in training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [{'Class': x, 'Count': y} for x, y in Counter(y_train).items()]\n",
    "distribution = pd.DataFrame(items).sort_values(['Class'])\n",
    "sns.barplot(x=distribution.Class, y=distribution.Count);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zarPJAOEkn58",
    "outputId": "5762193f-939f-419d-aea0-e6e680c9fb64"
   },
   "outputs": [],
   "source": [
    "Counter(y_train).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ijQ5qIKkn58",
    "outputId": "fffd362d-25bb-4c38-aee7-c7346d06151f"
   },
   "outputs": [],
   "source": [
    "Counter(y_test).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Le1sKFW9kn58"
   },
   "source": [
    "## Plot sample images with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "WCt1-X-mkn58",
    "outputId": "6b5c7578-bbc2-4c7c-b79b-9122ed89ca28"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15, 9))\n",
    "\n",
    "for i in range(50):\n",
    "    plt.subplot(5, 10, 1+i)\n",
    "    plt.title(y_train[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(x_train[i].reshape(28,28), cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWhzZFiJkn58"
   },
   "source": [
    "##  Preprocessing Data\n",
    "\n",
    "* Before we build our model, we need to prepare the data into the shape the network expected\n",
    "* More specifically, we will convert the labels (integers 0 to 9) to 1D numpy arrays of shape (10,) with elements 0s and 1s. \n",
    "* We also reshape the images from 2D arrays of shape (28,28) to 1D *float32* arrays of shape (784,) and then rescale their elements to values between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VOxXpikkn59"
   },
   "source": [
    "## Apply one-hot encoding on the labels\n",
    "\n",
    "We will change the way the labels are represented from numbers (0 to 9) to vectors (1D arrays) of shape (10, ) with all the elements set to 0 except the one which the label belongs to - which will be set to 1. For example:\n",
    "\n",
    "\n",
    "| original label | one-hot encoded label |\n",
    "|------|------|\n",
    "| 5 | [0 0 0 0 0 1 0 0 0 0] |\n",
    "| 7 | [0 0 0 0 0 0 0 1 0 0] |\n",
    "| 1 | [0 1 0 0 0 0 0 0 0 0] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_X_hLYUkn59",
    "outputId": "afaedf56-b48d-4fe2-eb78-c448ccebc77a"
   },
   "outputs": [],
   "source": [
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "print(\"First ten entries of y_train:\\n {}\\n\".format(y_train[0:10]))\n",
    "print(\"First ten rows of one-hot y_train:\\n {}\".format(y_train_encoded[0:10,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eTBZED0kn59",
    "outputId": "df1d161a-8252-4dc5-8855-5b0e1097130f"
   },
   "outputs": [],
   "source": [
    "print('y_train_encoded shape: ', y_train_encoded.shape)\n",
    "print('y_test_encoded shape: ', y_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lo3xfUCBkn59"
   },
   "source": [
    "## Reshape the images to 1D arrays\n",
    "\n",
    "Reshape the images from shape (28, 28) 2D arrays to shape (784, ) vectors (1D arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhj2fCiUkn59",
    "outputId": "53fb8aff-d3d0-4ef9-bca4-2365c6f13320"
   },
   "outputs": [],
   "source": [
    "# Before reshape:\n",
    "print('x_train:\\t{}'.format(x_train.shape))\n",
    "print('x_test:\\t\\t{}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGUdXbIzYSs4",
    "outputId": "f30f50a3-0333-4aea-f476-393a688af650"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=np.inf)\n",
    "print(\"{}\".format(x_train[2020]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfyHUMWlkn59",
    "outputId": "0a66450d-5ebe-4242-b2ba-bbad11d978c5"
   },
   "outputs": [],
   "source": [
    "# Reshape the images:\n",
    "x_train_reshaped = np.reshape(x_train, (60000, 784))\n",
    "x_test_reshaped = np.reshape(x_test, (10000, 784))\n",
    "\n",
    "# After reshape:\n",
    "print('x_train_reshaped shape: ', x_train_reshaped.shape)\n",
    "print('x_test_reshaped shape: ', x_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6XjR6qgkn5-"
   },
   "source": [
    "1. Each element in an image is a pixel value\n",
    "2. Pixel values range from 0 to 255\n",
    "3. 0 = White\n",
    "4. 255 = Black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mArIDYodYSs6"
   },
   "source": [
    "## Review unique values with set from 1st image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1joOK6Lkn5-",
    "outputId": "42f7f451-8ca4-4e88-e7fd-81ff1cf797fe"
   },
   "outputs": [],
   "source": [
    "print(set(x_train_reshaped[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eisT6z44kn5-"
   },
   "source": [
    "## Rescale the elements of the reshaped images\n",
    "\n",
    "Rescale the elements between [0 and 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WDlwS3hkn5_"
   },
   "outputs": [],
   "source": [
    "x_train_norm = x_train_reshaped.astype('float32') / 255\n",
    "x_test_norm = x_test_reshaped.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hIBXNcdWkn5_",
    "outputId": "cb2684da-8ab0-40f0-a2fd-e58d5eec27ef"
   },
   "outputs": [],
   "source": [
    "# Take a look at the first reshaped and normalized training image:\n",
    "print(set(x_train_norm[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwLvreknkn5_"
   },
   "source": [
    "## Creating the DNN Model\n",
    "\n",
    "* In this step, we first choose the network architecture for the model. Then we build.compile, train and evaulate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhUDwni8kn5_"
   },
   "source": [
    "## Specify the network architecture\n",
    "\n",
    "Below is the neural network architecture we will use today for classifying MNIST digits.\n",
    "\n",
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/DNN_architecture.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1bUu3ZNkn5_"
   },
   "source": [
    "## Build the DNN model\n",
    "\n",
    "We use a `Sequential` class defined in `Keras` to create our model. All the layers are going to be Dense layers. This means, like the figure shown above, all the nodes of a layer would be connected to all the nodes of the preceding layer i.e. densely connected.\n",
    "\n",
    "After the model is built, we view ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OdGUkHrkn5_"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(input_shape=[784], units=128, activation = tf.nn.relu,kernel_regularizer=tf.keras.regularizers.L2(0.001)),\n",
    "    Dense(name = \"output_layer\", units = 10, activation = tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpEmWU3mkn5_",
    "outputId": "08affdf5-a03d-4c93-d55a-b7239d6598af"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "hDxzWOj4kn6A",
    "outputId": "ed492562-ca67-4476-aa03-afcb9603cc15"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"mnist_model.png\", show_shapes=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aS7eRNF-kn6A"
   },
   "source": [
    "## Compile the DNN model\n",
    "\n",
    "In addition to setting up our model architecture, we also need to define which algorithm should the model use in order to optimize the weights and biases as per the given data. We will use stochastic gradient descent.\n",
    "\n",
    "We also need to define a loss function. Think of this function as the difference between the predicted outputs and the actual outputs given in the dataset. This loss needs to be minimized in order to have a higher model accuracy. That's what the optimization algorithm essentially does - it minimizes the loss during model training. For our multi-class classification problem, categorical cross entropy is commonly used.\n",
    "\n",
    "Finally, we will use the accuracy during training as a metric to keep track of as the model trains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YthKw2Dqkn6A"
   },
   "source": [
    "\n",
    "<b>tf.keras.optimizers.RMSprop</b><br> \n",
    " https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sikgiIMykn6A"
   },
   "source": [
    "\n",
    "<b>tf.keras.losses.CategoricalCrossentropy</b><br> \n",
    "  https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwX4nLPSkn6A"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',           \n",
    "               loss = 'categorical_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd1WOL9_kn6A"
   },
   "source": [
    "## Train the DNN model\n",
    "\n",
    "<b>tf.keras.model.fit</b><br>\n",
    "  https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YKqHTog9_dK"
   },
   "source": [
    "<b>tf.keras.callbacks.EarlyStopping</b><br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7sb2d_Skn6B",
    "outputId": "bd2d70e9-6720-4a78-fbc9-d388bd4c7816",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train_norm\n",
    "    ,y_train_encoded\n",
    "    ,epochs = 200\n",
    "    ,validation_split=0.20 \n",
    "    ,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)] \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDRd2ciikn6B"
   },
   "source": [
    "## Evaluate the DNN model\n",
    "\n",
    "In order to ensure that this is not a simple \"memorization\" by the machine, we should evaluate the performance on the test set. This is easy to do, we simply use the `evaluate` method on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CibH0FCVkn6B",
    "outputId": "babe13bb-cc47-4469-e4ce-8152a37439f7"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test_norm, y_test_encoded)\n",
    "print('test set accuracy: ', accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvOFT-E0kn6B"
   },
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFJWF2LQkn6B",
    "outputId": "f0500f0d-9144-4306-fc9c-9165060074de"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(x_test_norm)\n",
    "print('shape of preds: ', preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OnRD-4Mkn6B"
   },
   "source": [
    "Look at the first 25 - Plot test set images along with their predicted and actual labels to understand how the trained model actually performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "yHf320Lukn6B",
    "outputId": "f784673d-958a-409e-cd2d-34101c686174"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "\n",
    "start_index = 0\n",
    "\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    pred = np.argmax(preds[start_index + i])\n",
    "    actual = np.argmax(y_test_encoded[start_index + i])\n",
    "    col = 'g'\n",
    "    if pred != actual:\n",
    "        col = 'r'\n",
    "    plt.xlabel('i={} | pred={} | true={}'.format(start_index + i, pred, actual), color = col)\n",
    "    plt.imshow(x_test[start_index + i], cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "YSMY4EGukn6C",
    "outputId": "d516d37d-ea6b-4e54-c539-ea4e51c85c22"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enter the index value in place of the value 17 below for the prediction\n",
    "that you want to plot the probability scores for\n",
    "\"\"\"\n",
    "index = 17\n",
    "\n",
    "plt.plot(preds[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPnulsTmkn6C"
   },
   "source": [
    "##  Reviewing Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Inn4TNjSkn6C",
    "outputId": "d42bfe7d-1587-4ead-8b2b-616cafb77258"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DsCNcVRkn6C"
   },
   "source": [
    "## Plot performance metrics \n",
    "\n",
    "We use `Matplotlib` to create 2 plots--displaying the training and validation loss (resp. accuracy) for each (training) epoch side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvGSTvsGkn6C",
    "outputId": "7ec65a10-b618-4339-cefa-c266063b9c41"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Div9bSFkn6C"
   },
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "k9_OJwBFkn6C",
    "outputId": "a8778bd6-d392-44d6-e8f7-f7e9158be62d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
    "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1= model.predict(x_test_norm)\n",
    "pred1=np.argmax(pred1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUvBrDijkn6D"
   },
   "source": [
    "## Create the confusion matrix\n",
    "\n",
    "Let us see what the confusion matrix looks like. Using both `sklearn.metrics`. Then we visualize the confusion matrix and see what that tells us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fo5rd2ckn6D",
    "outputId": "18d2d221-1b30-416b-a476-0ed61e1d0689"
   },
   "outputs": [],
   "source": [
    "# Get the predicted classes:\n",
    "# pred_classes = model.predict_classes(x_train_norm)# give deprecation warning\n",
    "pred_classes = np.argmax(model.predict(x_test_norm), axis=-1)\n",
    "pred_classes;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbChYsktkn6D"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Correlation matrix that measures the linear relationships</b><br> \n",
    "    https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DH8JERwmkn6D",
    "outputId": "9f17f50d-e84a-4b45-eaa9-4767a3f9ed3d"
   },
   "outputs": [],
   "source": [
    "conf_mx = tf.math.confusion_matrix(y_test, pred_classes)\n",
    "conf_mx;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "RhTjgh9lkn6D",
    "outputId": "0ffa3274-993d-4d79-aa25-95c746be773a"
   },
   "outputs": [],
   "source": [
    "cm = sns.light_palette((260, 75, 60), input=\"husl\", as_cmap=True)\n",
    "df = pd.DataFrame(preds[0:20], columns = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "df.style.format(\"{:.2%}\").background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0L29J_9kn6E"
   },
   "source": [
    "## Visualize the confusion matrix\n",
    "\n",
    "We use code from chapter 3 of Hands on Machine Learning (A. Geron) (cf. https://github.com/ageron/handson-ml2/blob/master/03_classification.ipynb) to display a \"heat map\" of the confusion matrix. Then we normalize the confusion matrix so we can compare error rates. \n",
    "\n",
    "See https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch03.html#classification_chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qqNZrDGkn6E"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Correlation matrix that measures the linear relationships</b><br> \n",
    "    https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKaLoYl6kn6E"
   },
   "source": [
    "The confusion matrix looks quite good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peJdGyavkn6E"
   },
   "source": [
    "Looks like 28 fours were misclassified as nines (and 10 nines were classifed fours). We display some of these misclassfications along with exam of fours and nines that were correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoBQ7bxqkn6E"
   },
   "outputs": [],
   "source": [
    "def plot_digits(instances, pos, images_per_row=5, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    pos.imshow(image, cmap = 'binary', **options)\n",
    "    pos.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "VLEckUsCkn6F",
    "outputId": "9f0c2dab-3db7-4324-d247-e745a6703796"
   },
   "outputs": [],
   "source": [
    "cl_a, cl_b = 4, 9\n",
    "X_aa = x_test_norm[(y_test == cl_a) & (pred_classes == cl_a)]\n",
    "X_ab = x_test_norm[(y_test == cl_a) & (pred_classes == cl_b)]\n",
    "X_ba = x_test_norm[(y_test == cl_b) & (pred_classes == cl_a)]\n",
    "X_bb = x_test_norm[(y_test == cl_b) & (pred_classes == cl_b)]\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "p1 = plt.subplot(221)\n",
    "p2 = plt.subplot(222)\n",
    "p3 = plt.subplot(223)\n",
    "p4 = plt.subplot(224)\n",
    "\n",
    "plot_digits(X_aa[:25], p1, images_per_row=5);\n",
    "plot_digits(X_ab[:25], p2, images_per_row=5);\n",
    "plot_digits(X_ba[:25], p3, images_per_row=5);  \n",
    "plot_digits(X_bb[:25], p4, images_per_row=5);\n",
    "\n",
    "\n",
    "p1.set_title(f\"{cl_a}'s classified as {cl_a}'s\")\n",
    "p2.set_title(f\"{cl_a}'s classified as {cl_b}'s\")\n",
    "p3.set_title(f\"{cl_b}'s classified as {cl_a}'s\")\n",
    "p4.set_title(f\"{cl_b}'s classified as {cl_b}'s\")\n",
    "\n",
    "# plt.savefig(\"error_analysis_digits_plot_EXP1_valid\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKP9zbW6kn6G"
   },
   "source": [
    "## Analyzing the Activation Values of the Hidden Nodes\n",
    "\n",
    "We want to examine the contribution of the individual hidden nodes to the classifications made by the model. We first get the activation values of all the hidden nodes for each of the 60,000 training images and treat these *128 activations* as the features that determine the classification class. For the sake of comparison,  we also consider the *784 pixels* of each training image and determine the contribution of the individual pixels to the predicted classification class. \n",
    "\n",
    "Our goal is to use *box* and *scatter* plots to visualize how these features (*pixel* and *activation* values) correlate with the class labels. Because of the high dimension of the feature spaces, we apply *PCA decomposition* and *t-Distributed stochastic neighbor embedding* (`t-SNE`) to reduce the number of features in each case. \n",
    "\n",
    "We use the following two articles as reference\n",
    "\n",
    " * https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    " * https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWdXfzLDkn6G"
   },
   "source": [
    "1) Raw data is 60,000 X 784. Just do a scatter plot of col 1 vs col 2. Overlay the color coded classes. We  should not see any patterns since there is not much info in 2 cols to discriminate.\n",
    "\n",
    "2) PCA of raw data – as we discussed earlier. Plot PC1 vs PC2 with overlay. This should be ‘better’ since these 2 capture the info from all 784 cols.\n",
    "\n",
    "3) PCA of activation values – as we discussed earlier. This should be ‘better’ than the previous 2 since it has captured specific features of discrimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TB2I3nXkkn6H"
   },
   "source": [
    " ## Get the activation values of the hidden nodes\n",
    " \n",
    "To get the activation values of the hidden nodes, we need to create a new model, `activation_model`, that takes the same input as our current model but outputs the activation value of the hidden layer, i.e. of the hidden node. Then use the `predict` function to get the activation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDYg78ljkn6H",
    "outputId": "6bebf75b-91e6-4912-f500-65c82fc020da"
   },
   "outputs": [],
   "source": [
    "# Extracts the outputs of the 2 layers:\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "\n",
    "# Creates a model that will return these outputs, given the model input:\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "print(f\"There are {len(layer_outputs)} layers\")\n",
    "layer_outputs; # description of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cb6RjwIHkn6H",
    "outputId": "be65d965-69b1-4bfc-aa36-d2eba7e2998a"
   },
   "outputs": [],
   "source": [
    "# Get the outputs of all the hidden nodes for each of the 60000 training images\n",
    "activations = activation_model.predict(x_train_norm)\n",
    "hidden_layer_activation = activations[0]\n",
    "output_layer_activations = activations[1]\n",
    "hidden_layer_activation.shape   #  each of the 128 hidden nodes has one activation value per training image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJHVnXVOkn6H",
    "outputId": "d6fc3dd3-9cbc-45f7-d996-21cb5edeaa1b"
   },
   "outputs": [],
   "source": [
    "output_layer_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z87WPcOYkn6H",
    "outputId": "a9c0abf2-7878-42f7-b6ee-baa9e0081db6"
   },
   "outputs": [],
   "source": [
    "print(f\"The maximum activation value of the hidden nodes in the hidden layer is \\\n",
    "{hidden_layer_activation.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SizumOVikn6H",
    "outputId": "d453e7d6-56b6-4c66-faf4-aea790fb0a44"
   },
   "outputs": [],
   "source": [
    "# Some stats about the output layer as an aside...\n",
    "np.set_printoptions(suppress = True)  # display probabilities as decimals and NOT in scientific notation\n",
    "ouput_layer_activation = activations[1]\n",
    "print(f\"The output node has shape {ouput_layer_activation.shape}\")\n",
    "print(f\"The output for the first image are {ouput_layer_activation[0].round(4)}\")\n",
    "print(f\"The sum of the probabilities is (approximately) {ouput_layer_activation[0].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFf3WWuakn6I"
   },
   "source": [
    " ## Create a dataframe with the activation values and the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "PdDSFdaSkn6I",
    "outputId": "fefa7b76-fb61-44ae-976a-abfbcdc6e910"
   },
   "outputs": [],
   "source": [
    "#Get the dataframe of all the node values\n",
    "activation_data = {'actual_class':y_train}\n",
    "for k in range(0,128): \n",
    "    activation_data[f\"act_val_{k}\"] = hidden_layer_activation[:,k]\n",
    "\n",
    "activation_df = pd.DataFrame(activation_data)\n",
    "activation_df.head(15).round(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aayj64JEkn6I"
   },
   "source": [
    " ## Visualize the activation values with boxplots\n",
    " \n",
    "We get the activation values of the first hidden node and combine them with the corresponding class labels into a DataFrame. We use both `matplotlib` and `seaborn` to create boxplots from the dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvjUHIlykn6I"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>seaborn.boxplot</b><br> \n",
    "   ps://seaborn.pydata.org/generated/seaborn.boxplot.html\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "UJyRfUNXkn6I",
    "outputId": "ece0bd53-3209-4bd3-f290-fc4d65cb8e25"
   },
   "outputs": [],
   "source": [
    "# To see how closely the hidden node activation values correlate with the class labels\n",
    "# Let us use seaborn for the boxplots this time.\n",
    "bplot = sns.boxplot(y='act_val_0', x='actual_class', \n",
    "                 data=activation_df[['act_val_0','actual_class']], \n",
    "                 width=0.5,\n",
    "                 palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "VyiSMqnzYStN",
    "outputId": "777ea125-8b54-44da-f1a5-77b18e147239"
   },
   "outputs": [],
   "source": [
    "# displaying the range of activation values for each class labels\n",
    "activation_df.groupby(\"actual_class\")[\"act_val_0\"].apply(lambda x: [round(min(x.tolist()),2),\n",
    " round(max(x.tolist()),2)]).reset_index().rename(columns={\"act_val_0\": \"range_of_act_values\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQuOhPXEkn6I"
   },
   "source": [
    " ## Create a dataframe with the pixel values and class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "VQEq7TtNkn6J",
    "outputId": "9f066929-c5fd-4e09-cfdc-29d9e591f499"
   },
   "outputs": [],
   "source": [
    "#Get the dataframe of all the pixel values\n",
    "pixel_data = {'actual_class':y_train}\n",
    "for k in range(0,128): \n",
    "    pixel_data[f\"pix_val_{k}\"] = x_train_norm[:,k]\n",
    "pixel_df = pd.DataFrame(pixel_data)\n",
    "pixel_df.head(15).round(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QoatLqVqkn6J",
    "outputId": "d2548c25-b922-440a-b3de-b7089c017d00"
   },
   "outputs": [],
   "source": [
    "pixel_df.pix_val_77.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGG5iGTOkn6J",
    "outputId": "dd16f935-ed26-4410-f674-7010d5b5a997"
   },
   "outputs": [],
   "source": [
    "pixel_df.pix_val_78.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHNQJYbxkn6J"
   },
   "source": [
    "### Use a scatter plot to visualize the predicive power of the pixel values at two fixed locations in the image, i.e. how well the pixel values at two fixed locations in the image \"predict\" the class labels.\n",
    "\n",
    "We use a scatter plot to determine the correlation between the `pix_val_77` and `pix_val_78` values and the `actual_class` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "id": "6dg5gCQb-Ht7",
    "outputId": "96367edf-33a8-40d1-f175-a39459e42486"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "color = sns.color_palette(\"hls\", 10)\n",
    "sns.scatterplot(x=\"pix_val_77\", y=\"pix_val_78\", hue=\"actual_class\",  palette=color, data = pixel_df, legend=\"full\")\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BsbwZVMkn6J"
   },
   "source": [
    "## PCA Feature Reduction / Model Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA63_GUkkn6J"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>sklearn.decomposition.PCA</b><br> \n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKF8nrFFkn6J"
   },
   "source": [
    "## Use PCA decomposition to reduce the number of features from 784 features to 2 features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhSw_7Vokn6J"
   },
   "outputs": [],
   "source": [
    "# Separating out the features\n",
    "features = [*pixel_data][1:] # ['pix_val_0', 'pix_val_1',...]\n",
    "x = pixel_df.loc[:, features].values \n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LO5K6-7akn6K"
   },
   "outputs": [],
   "source": [
    "pixel_pca_df = pd.concat([principalDf, pixel_df[['actual_class']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "HGFswzktkn6K",
    "outputId": "abfd48cd-306a-490d-ef1f-f2d4c5a5a49a"
   },
   "outputs": [],
   "source": [
    "pixel_pca_df.head().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6hO43kckn6K",
    "outputId": "50494520-8c97-49a7-d441-661373a4eefc"
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Meh9cAFMkn6K"
   },
   "source": [
    "## Use a scatter plot to visualize the predictive power of the two principal component values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxuJNwF4kn6K"
   },
   "source": [
    "Using seaborn this time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "fQUlSV8nkn6K",
    "outputId": "f8e5092a-43fd-4e2c-d78f-49ff0cf386e1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"principal component 1\", y=\"principal component 2\",\n",
    "    hue=\"actual_class\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=pixel_pca_df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_o9TU0Qkn6L"
   },
   "source": [
    "### Use PCA decomposition to reduce the (activation) features from 128 (= num of hidden nodes) to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "WVN-ilL_kn6L",
    "outputId": "2922e825-67f1-4334-f26b-d6b756106ceb"
   },
   "outputs": [],
   "source": [
    "# Separating out the features\n",
    "features = [*activation_data][1:] # ['act_val_0', 'act_val_1',...]\n",
    "x = activation_df.loc[:, features].values \n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "principalDf.head().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tJXz-6sDkn6L",
    "outputId": "3e93ae92-a646-4c38-e3e1-58e9fd3d91be"
   },
   "outputs": [],
   "source": [
    "activation_pca_df = pd.concat([principalDf, activation_df[['actual_class']]], axis = 1)\n",
    "activation_pca_df.head().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goHqTkABkn6L",
    "outputId": "36afd99c-28e9-4493-9618-fcdb0e433e3c"
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkFFowrZYStT"
   },
   "source": [
    "The 2 principal components summed together 0.169 + 0.105 = .274 explained variance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35Fze3uLkn6L"
   },
   "source": [
    "## Use a scatter plot to visualize the predictive power of *two* principal component values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLs1s2Nfkn6L"
   },
   "source": [
    "Using seaborn this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "V4Cxy2S-kn6L",
    "outputId": "e2d5af59-5940-42e5-a059-2a5d7707e86a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"principal component 1\", y=\"principal component 2\",\n",
    "    hue=\"actual_class\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=activation_pca_df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYMQ20h7kn6M"
   },
   "source": [
    "### Use PCA decomposition to reduce the (activation) features from 128 (= num of hidden nodes) to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "LurC46Mfkn6M",
    "outputId": "5bf297c8-1229-4815-c93e-d8a19e6a4b69"
   },
   "outputs": [],
   "source": [
    "# Separating out the features\n",
    "features = [*activation_data][1:] # ['act_val_0', 'act_val_1',...]\n",
    "x = activation_df.loc[:, features].values \n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['pca-one', 'pca-two', 'pca-three'])\n",
    "principalDf.head(10).round(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ocCYpbxPkn6M",
    "outputId": "b6554fc0-c754-46ed-c91c-0e78dcbed889"
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_0S65NgYStU"
   },
   "source": [
    "The 3 principal components summed together 0.169 + 0.105 + 0.099 = 0.373 explained variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GVmnxrSEkn6M",
    "outputId": "eee06907-c65e-4743-d25e-25d1460fd7a8"
   },
   "outputs": [],
   "source": [
    "activation_pca_df = pd.concat([principalDf, activation_df[['actual_class']]], axis = 1)\n",
    "activation_pca_df.head().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMfy7Wqakn6M"
   },
   "source": [
    "## Use a scatter plot to visualize the predictive power of *three* principal component values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "kRb0E_9Jkn6M",
    "outputId": "657ba9c1-618b-4fa3-b67f-934df04cfd39",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# uncomment to to able to rotate the graph...\n",
    "# %matplotlib notebook   \n",
    "ax = plt.figure(figsize=(16,10)).gca(projection='3d')\n",
    "ax.scatter(\n",
    "    xs=activation_pca_df.loc[:,\"pca-one\"], \n",
    "    ys=activation_pca_df.loc[:,\"pca-two\"], \n",
    "    zs=activation_pca_df.loc[:,\"pca-three\"], \n",
    "    c=activation_pca_df.loc[:,\"actual_class\"], \n",
    "    cmap='tab10'\n",
    ")\n",
    "ax.set_xlabel('pca-one')\n",
    "ax.set_ylabel('pca-two')\n",
    "ax.set_zlabel('pca-three')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFpWdY_Lkn6N"
   },
   "source": [
    "## Use t-Distributed Stochastic Neighbor Embedding (**t-SNE**)  to reduce the (activation) features from 128 (= num of hidden nodes) to 2\n",
    "\n",
    "`t-Distributed Stochastic Neighbor Embedding` (**t-SNE**) is another technique for dimensionality reduction and is particularly well suited for the visualization of high-dimensional datasets. This time we only use the first 10,000 training images (N=10000) since the technique is computationally expensive.\n",
    "\n",
    "See http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ie056N_mkn6N"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>sklearn.manifold.TSNE</b><br> \n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YADYnVMKYStW",
    "outputId": "a6464916-a9aa-4d78-d98e-9359a62865f7"
   },
   "outputs": [],
   "source": [
    "activation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIyXEjBUkn6N",
    "outputId": "2cb8c90c-53e0-47de-a7ae-df3f2967d0b1"
   },
   "outputs": [],
   "source": [
    "N=60000\n",
    "activation_df_subset = activation_df.iloc[:N].copy()\n",
    "activation_df_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FmVarnbLkn6N",
    "outputId": "a33d5209-50d7-4a70-e2e8-81924f20b9c4"
   },
   "outputs": [],
   "source": [
    "data_subset = activation_df_subset[features].values\n",
    "data_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jj_G_gBHkn6N",
    "outputId": "d46df8c2-77b0-4ded-a031-0cf0971c3aab"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(data_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TH1i2GexYStX"
   },
   "outputs": [],
   "source": [
    "tsne_results = (tsne_results - tsne_results.min()) / (tsne_results.max() - tsne_results.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "id": "ZOgoyuEoYStX",
    "outputId": "057ec3d3-90d5-4402-e530-76f81b818a58"
   },
   "outputs": [],
   "source": [
    "# https://github.com/ageron/handson-ml2/blob/master/17_autoencoders_and_gans.ipynb\n",
    "# adapted from https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html\n",
    "# plt.figure(figsize=(10, 8))\n",
    "cmap = plt.cm.tab10\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.scatter(tsne_results[:,0],tsne_results[:,1], c=y_train, s=10, cmap=cmap)\n",
    "\n",
    "image_positions = np.array([[1., 1.]])\n",
    "for index, position in enumerate(tsne_results):\n",
    "    dist = np.sum((position - image_positions) ** 2, axis=1)\n",
    "    if np.min(dist) > 0.02: # if far enough from other images\n",
    "        image_positions = np.r_[image_positions, [position]]\n",
    "        imagebox = mpl.offsetbox.AnnotationBbox(\n",
    "            mpl.offsetbox.OffsetImage(x_train[index], cmap=\"binary\"),\n",
    "            position, bboxprops={\"edgecolor\": cmap(y_train[index]), \"lw\": 2})\n",
    "        plt.gca().add_artist(imagebox)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49-8XWfMkn6N"
   },
   "source": [
    "## Reducing dimensionality of the data with Random Forests.\n",
    "\n",
    "We create a Random Forest Classifier (with the default 100 trees) and use it to find the relative importance of the 784 features (pixels) in the training set. We produce a heat map to visual the relative importance of the features (using code from Hands On Machine Learning by A. Geron). Finally, we select the 70 most important feature (pixels) from the training, validation and test images to test our 'best' model on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQB0eG7rkn6N"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>sklearn.ensemble.RandomForestClassifier</b><br> \n",
    "  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41iidtM8kn6O",
    "outputId": "3c1e0fb8-7519-4cb2-f6ec-e401ac36e998"
   },
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rnd_clf.fit(x_train_norm,y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "N84jCjrHkn6O",
    "outputId": "e51916a6-1db3-4e78-c7f0-e6edb1501312"
   },
   "outputs": [],
   "source": [
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = 'hot',\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plot_digit(rnd_clf.feature_importances_)\n",
    "cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(), rnd_clf.feature_importances_.max()])\n",
    "cbar.ax.set_yticklabels(['Not important', 'Very important'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vj8H8MyNkn6O",
    "outputId": "9e218e0d-e3b3-470c-9597-488844811da1"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "n = 70\n",
    "imp_arr = rnd_clf.feature_importances_\n",
    "idx = (-imp_arr).argsort()[:n]          # get the indices of the 70 \"most important\" features/pixels\n",
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bjSKY1Nkn6O",
    "outputId": "08077bd4-ff6e-4344-ee72-c3f65004f2c2"
   },
   "outputs": [],
   "source": [
    "# Create training and test images using just the 70 pixel locations obtained above\n",
    "train_images_sm = x_train_norm[:,idx]\n",
    "test_images_sm = x_test_norm[:,idx]\n",
    "train_images_sm.shape, test_images_sm.shape # the reduced images have dimension 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UC-WiBYxkn6O"
   },
   "source": [
    "## Visualize the 70 pixels\n",
    "We convert the array of indexes to ordered pairs and plot them as red circles on the second training image. These are the features (pixels) we train our neural network on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLv2ajRAkn6O"
   },
   "outputs": [],
   "source": [
    "# to convert an index n, 0<= n < 784\n",
    "def pair(n,size):\n",
    "    x = n//size \n",
    "    y = n%size\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "CkkoeKprkn6O",
    "outputId": "122c04c6-4bd7-4313-f093-c62366445674"
   },
   "outputs": [],
   "source": [
    "plt.imshow(x_train_norm[1].reshape(28,28),cmap='binary')\n",
    "x, y = np.array([pair(k,28) for k in idx]).T\n",
    "plt.scatter(x,y,color='red',s=20)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BhUDwni8kn5_",
    "gMfy7Wqakn6M"
   ],
   "machine_shape": "hm",
   "name": "MSDS458_Assignment_01_20210619_DEV_v19_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
