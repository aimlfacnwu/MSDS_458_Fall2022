{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxE1PPwJi9zS",
        "outputId": "e39f5a0f-0859-4db7-e6fe-262ca9193e2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_nlp\n",
            "  Downloading keras_nlp-0.4.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.5/337.5 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (23.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (2.11.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (1.4.0)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (1.21.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (1.51.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (2.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (23.1.21)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (2.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (0.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (2.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (57.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (0.30.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (2.11.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (3.19.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (1.15.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (15.0.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (4.4.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-text->keras_nlp) (0.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow->keras_nlp) (0.38.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (2.25.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (2.16.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (6.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (2022.12.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (3.12.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras_nlp) (3.2.2)\n",
            "Installing collected packages: tensorflow-text, keras_nlp\n",
            "Successfully installed keras_nlp-0.4.0 tensorflow-text-2.11.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import keras\n",
        "!pip3 install keras_nlp\n",
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "BATCH_SIZE = 64\n",
        "SEQ_LEN = 128\n",
        "MIN_TRAINING_SEQ_LEN = 450\n",
        "\n",
        "# Model\n",
        "EMBED_DIM = 256\n",
        "FEED_FORWARD_DIM = 256\n",
        "NUM_HEADS = 3\n",
        "NUM_LAYERS = 2\n",
        "VOCAB_SIZE = 5000  # Limits parameters in model.\n",
        "\n",
        "# Training\n",
        "EPOCHS = 6\n",
        "\n",
        "# Inference\n",
        "NUM_TOKENS_TO_GENERATE = 80\n"
      ],
      "metadata": {
        "id": "zzF-t2Ssi_Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.get_file(\n",
        "    origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n",
        "\n",
        "# Load simplebooks-92 train set and filter out short lines.\n",
        "raw_train_ds = (\n",
        "    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")\n",
        "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(buffer_size=256)\n",
        ")\n",
        "\n",
        "# Load simplebooks-92 validation set and filter out short lines.\n",
        "raw_val_ds = (\n",
        "    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/valid.txt\")\n",
        "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ353EmnjOBE",
        "outputId": "c0a2faf0-cf6f-4ebe-bd84-b1cf2ee09daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\n",
            "282386239/282386239 [==============================] - 10s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train tokenizer vocabulary\n",
        "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "    raw_train_ds,\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    lowercase=True,\n",
        "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
        ")\n"
      ],
      "metadata": {
        "id": "VldEZx5XjoWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    sequence_length=SEQ_LEN,\n",
        "    lowercase=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "ED6CK5KIjua4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# packer adds a start token\n",
        "start_packer = keras_nlp.layers.StartEndPacker(\n",
        "    sequence_length=SEQ_LEN,\n",
        "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess(inputs):\n",
        "    outputs = tokenizer(inputs)\n",
        "    features = start_packer(outputs)\n",
        "    labels = outputs\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "# Tokenize and split into train and label sequences.\n",
        "train_ds = raw_train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")\n",
        "val_ds = raw_val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")\n"
      ],
      "metadata": {
        "id": "8-JVoD6njy1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
        "# Embedding.\n",
        "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    sequence_length=SEQ_LEN,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")\n",
        "x = embedding_layer(inputs)\n",
        "# Transformer decoders.\n",
        "for _ in range(NUM_LAYERS):\n",
        "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
        "        num_heads=NUM_HEADS,\n",
        "        intermediate_dim=FEED_FORWARD_DIM,\n",
        "    )\n",
        "    x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n",
        "# Output.\n",
        "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
        "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])\n"
      ],
      "metadata": {
        "id": "BOMFhXXYkVy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zw1CQq4kb9_",
        "outputId": "0c7fd3b3-7c96-4e22-891e-20cc075038ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, None, 256)        1312768   \n",
            " g (TokenAndPositionEmbeddin                                     \n",
            " g)                                                              \n",
            "                                                                 \n",
            " transformer_decoder (Transf  (None, None, 256)        394749    \n",
            " ormerDecoder)                                                   \n",
            "                                                                 \n",
            " transformer_decoder_1 (Tran  (None, None, 256)        394749    \n",
            " sformerDecoder)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 5000)        1285000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,387,266\n",
            "Trainable params: 3,387,266\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8he35MszkjUz",
        "outputId": "65acf477-e9ec-49bd-ff2c-e16e4676100f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "3169/3169 - 341s - loss: 4.5635 - perplexity: 96.2858 - val_loss: 4.1107 - val_perplexity: 61.6287 - 341s/epoch - 107ms/step\n",
            "Epoch 2/6\n",
            "3169/3169 - 230s - loss: 4.0483 - perplexity: 57.5224 - val_loss: 3.9932 - val_perplexity: 54.6582 - 230s/epoch - 73ms/step\n",
            "Epoch 3/6\n",
            "3169/3169 - 229s - loss: 3.9335 - perplexity: 51.2811 - val_loss: 3.9180 - val_perplexity: 50.7189 - 229s/epoch - 72ms/step\n",
            "Epoch 4/6\n",
            "3169/3169 - 229s - loss: 3.8706 - perplexity: 48.1511 - val_loss: 3.8804 - val_perplexity: 48.8415 - 229s/epoch - 72ms/step\n",
            "Epoch 5/6\n",
            "3169/3169 - 230s - loss: 3.8283 - perplexity: 46.1560 - val_loss: 3.8545 - val_perplexity: 47.5279 - 230s/epoch - 73ms/step\n",
            "Epoch 6/6\n",
            "3169/3169 - 230s - loss: 3.7953 - perplexity: 44.6590 - val_loss: 3.8097 - val_perplexity: 45.5479 - 230s/epoch - 73ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd3a3cb6340>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt_tokens = tf.convert_to_tensor([tokenizer.token_to_id(\"[BOS]\")])"
      ],
      "metadata": {
        "id": "u3X1gohNkm48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token_logits_fn(inputs):\n",
        "    cur_len = inputs.shape[1]\n",
        "    output = model(inputs)\n",
        "    return output[:, cur_len - 1, :]  # return next token logits\n"
      ],
      "metadata": {
        "id": "eeHm6vX9knCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = keras_nlp.utils.greedy_search(\n",
        "    token_logits_fn,\n",
        "    prompt_tokens,\n",
        "    max_length=NUM_TOKENS_TO_GENERATE,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Greedy search generated text: \\n{txt}\\n\")\n"
      ],
      "metadata": {
        "id": "-bI2OiAuktuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f70a886-d0e8-464a-d4d3-26da7be9ec08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy search generated text: \n",
            "b'[BOS] \" i \\' m going to be a good deal of trouble , \" said the old man . \" i \\' m going to be a good man , and i \\' m going to be a good man . i \\' m going to be a good man , and i \\' m going to be a good man . i \\' m going to be a good man , and i \\' m going to be'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = keras_nlp.utils.beam_search(\n",
        "    token_logits_fn,\n",
        "    prompt_tokens,\n",
        "    max_length=NUM_TOKENS_TO_GENERATE,\n",
        "    num_beams=10,\n",
        "    from_logits=True,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Beam search generated text: \\n{txt}\\n\")\n"
      ],
      "metadata": {
        "id": "Ebylb-vpku4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e6b17c-2f64-4af2-daff-6b4ffa875b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam search generated text: \n",
            "b'[BOS] \" i don \\' t know what i \\' m going to do . i don \\' t know what i \\' m going to do . i don \\' t know what i \\' m going to do . i don \\' t know what i \\' m going to do . i don \\' t know what i \\' m going to do . i don \\' t know what i \\' m going to do .'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = keras_nlp.utils.random_search(\n",
        "    token_logits_fn,\n",
        "    prompt_tokens,\n",
        "    max_length=NUM_TOKENS_TO_GENERATE,\n",
        "    from_logits=True,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Random search generated text: \\n{txt}\\n\")\n"
      ],
      "metadata": {
        "id": "GiVhira9ku7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c007677-ac2a-4d9b-8474-7594ce99ac5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random search generated text: \n",
            "b'[BOS] \" we may flemington india devils , though indeed , had this trouble with the children , \" announced annie ; \" or \" them , kneeled in high regards the sea in her mind of england , and ornament except your loath - operation may have been steadily decorated in war . i am fearfully received from a concurmur'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = keras_nlp.utils.top_k_search(\n",
        "    token_logits_fn,\n",
        "    prompt_tokens,\n",
        "    max_length=NUM_TOKENS_TO_GENERATE,\n",
        "    k=10,\n",
        "    from_logits=True,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Top-K search generated text: \\n{txt}\\n\")\n"
      ],
      "metadata": {
        "id": "2ENBo-pFku9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a529fd-5783-4c3b-e040-a816042cfe4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-K search generated text: \n",
            "b'[BOS] in the morning the boys were dressed in the morning as they were riding at the side of the wagon , and there was no time to ride back at night , and it was not a very much better to take the rider . when the girls got behind their coats and walked to the wagon , and were all ready and waiting . they had the best of them , and were to ride over ,'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = keras_nlp.utils.top_p_search(\n",
        "    token_logits_fn,\n",
        "    prompt_tokens,\n",
        "    max_length=NUM_TOKENS_TO_GENERATE,\n",
        "    p=0.5,\n",
        "    from_logits=True,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Top-P search generated text: \\n{txt}\\n\")\n"
      ],
      "metadata": {
        "id": "AwlnouUeknFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc29c16-8101-4376-d2d4-3c45c2292d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-P search generated text: \n",
            "b'[BOS] she had done all the same , and she had to be married . she had taken a long project of the house , and she was in a new place . she had never seen such a person . she had not only got a dollar , but the more she liked to be married . it was only a small , well - worn place , and her heart was glad to see'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TopKTextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n",
        "\n",
        "    def __init__(self, k):\n",
        "        self.k = k\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        output_tokens = keras_nlp.utils.top_k_search(\n",
        "            token_logits_fn,\n",
        "            prompt_tokens,\n",
        "            max_length=NUM_TOKENS_TO_GENERATE,\n",
        "            k=self.k,\n",
        "            from_logits=True,\n",
        "        )\n",
        "        txt = tokenizer.detokenize(output_tokens)\n",
        "        print(f\"Top-K search generated text: \\n{txt}\\n\")\n",
        "\n",
        "\n",
        "text_generation_callback = TopKTextGenerator(k=10)\n",
        "# Dummy training loop to demonstrate callback.\n",
        "model.fit(train_ds.take(1), verbose=2, epochs=2, callbacks=[text_generation_callback])\n"
      ],
      "metadata": {
        "id": "am5Z9i_aknID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada0b12e-22c6-42ec-c3bd-4a4fee9f2e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] when they had taken the little party to the camp on the bouquet and the confederate captain . it was late now , indeed . when it was completed i was in command of the party . the girls were all in the neighbourhood , and i could see that there was no doubt , for i was not at all the company , and in the morning i saw'\n",
            "\n",
            "1/1 - 9s - loss: 3.8761 - perplexity: 48.3112 - 9s/epoch - 9s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" i am a little too , \" the doctor said . \" it was a good deal like you , but i am not sure , and you see how much of it . i don \\' t think i have any of you or any more . but the man who will tell me about you now has to tell him about it ; the man , who is a man in the country of the city'\n",
            "\n",
            "1/1 - 9s - loss: 3.7646 - perplexity: 43.4369 - 9s/epoch - 9s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd2852583d0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text_generation_callback = TopKTextGenerator(k=10)\n",
        "# Dummy training loop to demonstrate callback.\n",
        "model.fit(train_ds.take(10), verbose=2, epochs=2, callbacks=[text_generation_callback])"
      ],
      "metadata": {
        "id": "e5HccgJZk-as",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d67ae3d6-19d7-4c34-861b-c935882c3e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b\"[BOS] he was a tall and nobleman with his hand , with an old and noble youthful hand , and in the midst of his great adversal in the old man ' s eyes , and he could not have known his brother ' s name as his son as a son of a man , but had he , and the chief had a strong , well , was so indul\"\n",
            "\n",
            "10/10 - 10s - loss: 3.7872 - perplexity: 44.3147 - 10s/epoch - 986ms/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" it \\' s the best , \" continued mr . dave . \" they were to be a peck of water , and they made their way in the direction of the river . they were all in the river bank at a spot where their boats was going to the north , but we got on a short distance , and i made a little shady creek in two days , and'\n",
            "\n",
            "10/10 - 10s - loss: 3.7339 - perplexity: 41.9981 - 10s/epoch - 961ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd285df6340>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TopKTextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n",
        "\n",
        "    def __init__(self, k):\n",
        "        self.k = k\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        output_tokens = keras_nlp.utils.top_k_search(\n",
        "            token_logits_fn,\n",
        "            prompt_tokens,\n",
        "            max_length=NUM_TOKENS_TO_GENERATE,\n",
        "            k=self.k,\n",
        "            from_logits=True,\n",
        "        )\n",
        "        txt = tokenizer.detokenize(output_tokens)\n",
        "        print(f\"Top-K search generated text: \\n{txt}\\n\")\n",
        "\n",
        "\n",
        "text_generation_callback = TopKTextGenerator(k=10)\n",
        "# Dummy training loop to demonstrate callback.\n",
        "for i in range(10):\n",
        "  model.fit(train_ds.take(1), verbose=2, epochs=2, callbacks=[text_generation_callback])\n",
        "\n"
      ],
      "metadata": {
        "id": "HdzcqxB7k-dA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62d122e0-512d-42d3-9601-b5ba2c7c7292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" the shrewd man was the first to be a big , and was quite sure , \" said the girl . \" i have not been so happy as to be , and i am glad to know it . \" and there were no great things on the landlord of his uncle ; and the little old man , who had been a good man . he had not seen the world'\n",
            "\n",
            "1/1 - 14s - loss: 3.4981 - perplexity: 33.1378 - 14s/epoch - 14s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] the two lads were not to be afraid . the latter were to see that the scientists were the same , that a subsequently of civilization , the scotch and compound . the boys in a position in which he had been in the colonial , and , in his present century , were the colonialist generals'\n",
            "\n",
            "1/1 - 9s - loss: 3.5452 - perplexity: 34.6899 - 9s/epoch - 9s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] he now went up , thinking he would try , but he was not going to see if he was going to be sure that it would be hard if he could find out what he could get to do . so he got out of doors and closed in his pockets and shut out . and when he was out , the door was closed , he could see that he must be there ; and he would'\n",
            "\n",
            "1/1 - 9s - loss: 3.4804 - perplexity: 32.6338 - 9s/epoch - 9s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] the skipper of the thoroughed the pinnacle , who was a man of prolonged , and the screanting of an army on the right side of the boers in the supernatural bureau . the priord , was a promplished a pearlse'\n",
            "\n",
            "1/1 - 9s - loss: 4.0881 - perplexity: 59.8835 - 9s/epoch - 9s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] when the two girls were seated in the little cabin and were seated , and a group of conning - wheel - drivers were placed in the middle of the road leading to the east and on the west . there was a number of other people who were standing there , and they knew that there was a curve of the convent that arrest than any of those who'\n",
            "\n",
            "1/1 - 9s - loss: 3.7767 - perplexity: 43.9611 - 9s/epoch - 9s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] the skirt was a precipitation of the boys in order to get a view of it , and it was necessary to communicate it to the problem of a man who had been the first and one he would have to do the same thing . he had been the first to be made in a skilful manner ; but the boys could'\n",
            "\n",
            "1/1 - 9s - loss: 3.9097 - perplexity: 50.0256 - 9s/epoch - 9s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" you are to be a little surprised . your mother and you were a good girl to be the leader of them , and the two girls were very glad to be the one for the sake of the rest . you have to do so . you are very good to them all to be done , and we must help each other and make their own ends . if you will help each other if you'\n",
            "\n",
            "1/1 - 9s - loss: 3.9557 - perplexity: 52.2659 - 9s/epoch - 9s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" that \\' s just what you \\' re going to say to my own father ! \" she said . \" you must all the world in the world are not a very different thing ! and the other thing is to be done , \" the king went on to his own house ; but the king was not quite willing to go , and the old man was afraid that the little one would bring the'\n",
            "\n",
            "1/1 - 8s - loss: 3.6559 - perplexity: 38.7549 - 8s/epoch - 8s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] the two girls were as well pleased as their parents . the girls , too , were a tall one , and a large , broad shoulders , with its curly head , and arms , so large and large and beautifully shaped , and the two little girls wore , but all were very large . one of these boys , who was a young person , was a tall , young fellow - - a'\n",
            "\n",
            "1/1 - 8s - loss: 3.7522 - perplexity: 42.7111 - 8s/epoch - 8s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] there was another in the little house . a few children were very fond of each other . they were all very fond of food ; they were often fond of food and food and water . sometimes they were very fond of food , sometimes . sometimes one often they would sometimes have plenty of eggs for a nest in the nest , and sometimes one day he would have to spend the winter in summer . sometimes'\n",
            "\n",
            "1/1 - 9s - loss: 3.7340 - perplexity: 41.9513 - 9s/epoch - 9s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" i do not believe that , \" replied the other , \" because , as the matter has been arranged ; but there is no doubt he will not have been more correct , as we know that we cannot do what i wish , for i have been able to do it for you , and that it will not be the best to be made for the skirls . i can hardly be'\n",
            "\n",
            "1/1 - 9s - loss: 3.6027 - perplexity: 36.8414 - 9s/epoch - 9s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" i think you \\' d better be here , \" said the old man with satisfaction . \" i \\' m glad i \\' m not going to have it on , and i \\' d better go on a good road . the doctor is the first to be , and i \\' d have been so glad that we had gone for the rest , and that i \\' d better go to arnold'\n",
            "\n",
            "1/1 - 10s - loss: 3.8153 - perplexity: 45.5052 - 10s/epoch - 10s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] it was the same , as the two men , to whom they had been sent in to the northward ; but they had not been able to obtain their own information , and was the only one to learn whether he had not yet learned to be . they had been so much to say , and they did not seem that there was not a man in the army ; and had been sent down with'\n",
            "\n",
            "1/1 - 9s - loss: 3.4963 - perplexity: 33.0673 - 9s/epoch - 9s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] there were no one else in the streets , and the men in the street were the houses in the village . the soldiers and soldiers were the man who had been the man of the town ; and there were not only twenty men , but the soldiers did not know what was said . the people were so eager to hear the news . they were all on the road , and the men were very glad'\n",
            "\n",
            "1/1 - 9s - loss: 3.8484 - perplexity: 47.0208 - 9s/epoch - 9s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b\"[BOS] he had no time to think of that , though he was a manly lad , and he was always a little fellow , and though his father was so happy , in his father ' s mind being well - disposed to a man with a man and a surpulentative , to a very bad one , and his father had so far more important a step in his father , that\"\n",
            "\n",
            "1/1 - 9s - loss: 3.6419 - perplexity: 38.2329 - 9s/epoch - 9s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" yes , \" she said . \" i have told you that you will know what you will . i have no more of you to do so . i believe , i know . the king is to take his wife away . i can see her . he is the only daughter of the queen , and he has no husband . they would do him good for her ; and if he would get the'\n",
            "\n",
            "1/1 - 9s - loss: 4.0373 - perplexity: 56.7123 - 9s/epoch - 9s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] but the preparation of the bronx was in great numbers of the men who were so incessantly in the constituted a great number of men who had fought so stoutly . the convoy were so furious that they fell to the ground . a small crowd was surmised with a number of men , but there were few men who were'\n",
            "\n",
            "1/1 - 9s - loss: 3.5267 - perplexity: 34.1120 - 9s/epoch - 9s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] he was a very prostrater ; he did not think , for the next four hours , and then , after all , and he came back to the spot where his father had gone ; and so that , if he had come , he would come in and get his way through the woodland , he would not be able to keep a sharp lookout . he had been thinking that'\n",
            "\n",
            "1/1 - 9s - loss: 3.8812 - perplexity: 48.5156 - 9s/epoch - 9s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" i am not going to leave it , and will do so much more to do with it . it is only for two o \\' clock . we will get to go on board the steamer for the first time , and make a change will at once and see if they come to her there , and we shall go back to the screened . we are not to be found ; we'\n",
            "\n",
            "1/1 - 9s - loss: 3.8076 - perplexity: 45.2250 - 9s/epoch - 9s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" the man is not a great man as a man , but i can assure him that he is so fond of himself in his case ; and he is not sure that the king has been slain by his sword ; and , in the first place , the second time of that , that was so far off ; that his father , as well as to his own father , he would be able to'\n",
            "\n",
            "1/1 - 9s - loss: 3.7736 - perplexity: 43.7495 - 9s/epoch - 9s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TopKTextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n",
        "\n",
        "    def __init__(self, k):\n",
        "        self.k = k\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        output_tokens = keras_nlp.utils.top_k_search(\n",
        "            token_logits_fn,\n",
        "            prompt_tokens,\n",
        "            max_length=NUM_TOKENS_TO_GENERATE,\n",
        "            k=self.k,\n",
        "            from_logits=True,\n",
        "        )\n",
        "        txt = tokenizer.detokenize(output_tokens)\n",
        "        print(f\"Top-K search generated text: \\n{txt}\\n\")\n",
        "\n",
        "\n",
        "text_generation_callback = TopKTextGenerator(k=10)\n",
        "# Dummy training loop to demonstrate callback.\n",
        "for i in range(10):\n",
        "  model.fit(val_ds.take(1), verbose=2, epochs=2, callbacks=[text_generation_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0QpMK6cjnsv",
        "outputId": "e6c0fdd5-d094-4ae4-bd40-71db89bcbead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] the boys were to wait for some time , and then they went back to the place where they had left . it was the same , and the other two boys were not to be disturbed by the rest , who did not know that their father was not to blame for the doctor . they could have heard the others say they could not understand that they were too weak and helpless . they were too young'\n",
            "\n",
            "1/1 - 4s - loss: 3.8351 - perplexity: 46.7020 - 4s/epoch - 4s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" it is a great time to have the right of this , and a hundred times , \" said he . \" i have been there many days to come here to - day , and you know that it is a little too good for us to be out . we \\' re glad to think about the whole affair . it is just the right thing you can do . it was a little better off'\n",
            "\n",
            "1/1 - 4s - loss: 3.7596 - perplexity: 43.3168 - 4s/epoch - 4s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] when the day was over she found the bronx of the constable , and her brother was sitting with it all his father \\' s clothes . \" now , \" said his mother , and he said , \" you have a chance that the poor man will have to buy his money with it . \" and i shall be sure to have the money with him . \" she went on with'\n",
            "\n",
            "1/1 - 4s - loss: 3.6297 - perplexity: 38.0565 - 4s/epoch - 4s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] in a few seconds the two boys stood motionless . they stood motionless with their hands as he did so . they were almost at the same time they were on the side of the side and were the two lads . their faces did not speak in the middle . they were as silent and silent as the infamous tones , while they were gazing at them , and they had not been'\n",
            "\n",
            "1/1 - 4s - loss: 3.4656 - perplexity: 32.3060 - 4s/epoch - 4s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] the next day she was to take a few steps to her room . a little girl had seen two girls coming along the floor . she had heard a little scrape - curls in the back door of the big house in that room . there were many of them . one had a cushion of straw and two were so large and very large to sit and a large one that'\n",
            "\n",
            "1/1 - 4s - loss: 3.2835 - perplexity: 26.9292 - 4s/epoch - 4s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] the scolding of the bank , in the meantime , was a small piece of paper , and a large piece of chouen and a large quantity of conspicy and a bucking of clay . the other was tied around to the pantry , which was the tickets , the buckles , with its bucking'\n",
            "\n",
            "1/1 - 4s - loss: 3.0935 - perplexity: 22.2662 - 4s/epoch - 4s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" you \\' re all right , \" was the reply . \" there are two people around them and they are two . they have some little ones about two hundred children . they have to take a little walks , and you must have to stay . they may take their horses . they don \\' t go in their horses and get a cart . they have their baskets in the wagon . they'\n",
            "\n",
            "1/1 - 4s - loss: 2.8999 - perplexity: 18.3428 - 4s/epoch - 4s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" i am afraid i was , \" she said , as she sat down on the grass , looking over her shoulder , but she did not see . \" and there are two children about her . you will never have to talk about . it \\' s too much to talk about people about . but you \\' ll have a lot of talking about her , for she would have a chance to answer a'\n",
            "\n",
            "1/1 - 4s - loss: 2.7045 - perplexity: 15.0803 - 4s/epoch - 4s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" you \\' re all of you \\' re going to take a curly , \" explained mr . jardine . \" you \\' re very much better than you \\' re in the same time , and then i \\' ll have to do it all to . you \\' re as much too . you see , laddie , as well as i can , i am so glad you will be'\n",
            "\n",
            "1/1 - 4s - loss: 2.5092 - perplexity: 12.3976 - 4s/epoch - 4s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" now , \" explained the old man ; \" my father \\' s uncle , and i have no doubt they have to - night . it is a very big , old boy of all i can \\' t take it in to make up my mind . i have the big dog \\' s head in front of the dog , and i don \\' t want to talk over to . i \\' ll ask'\n",
            "\n",
            "1/1 - 4s - loss: 2.3163 - perplexity: 10.2164 - 4s/epoch - 4s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] it was not long before the king could see them lying in the grass . his mother was very fond of the little children . but he was seldom called them . he was often used to catch the little dog . he wanted to play with them , and he seldom played in a wild little while , and though the children did not play , though he was so fond of singing that the little boys could'\n",
            "\n",
            "1/1 - 4s - loss: 2.1290 - perplexity: 8.4661 - 4s/epoch - 4s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" you have no right to do so ! \" cried the doctor . \" you have nothing more to do to make warren . and now let me come to the end of my life , and send my little salmon over the country . i am going to take chances of getting up in the hills , which i am getting up from my store here . if i will get to - night'\n",
            "\n",
            "1/1 - 4s - loss: 1.9495 - perplexity: 7.0700 - 4s/epoch - 4s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b\"[BOS] then sir lordon went to his place , and came over to see if there were not a few children . they werenhitched , but there was no place , for the doctor had spoken . the doctor wanted to see that the doctor was doing so , but he was not afraid to do something to eat . and though i had done nothing wrong to say , ' what ' s\"\n",
            "\n",
            "1/1 - 4s - loss: 1.7789 - perplexity: 5.9560 - 4s/epoch - 4s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] it may be something about the little girls who were very fond of the sport , though , though they were not afraid , if they had not spoken of them . they had not spoken , as the boys had done , as they had done their part in the afternoon , as it would make them run , as if they had done . it was harder than the boys were getting in , and the boys'\n",
            "\n",
            "1/1 - 4s - loss: 1.6171 - perplexity: 5.0612 - 4s/epoch - 4s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" yes , but now let the girls have a lot of wolves and other children , \" explained suently . \" it was the case of mr . martin , but he had not done so much , and the children were so big that , although they did not care so , as to the others could see it , as far down the street , so far , they could see the little'\n",
            "\n",
            "1/1 - 4s - loss: 1.4633 - perplexity: 4.3376 - 4s/epoch - 4s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" yes , \" andy had said to herself , \" it was a very big one , but it wasn \\' t really the right thing . of course you could do right now , laddie . i was quite frightened of you , and he was hurt . but i did not have to ask you , for i could make no sound of this . i am very much frightened . i am'\n",
            "\n",
            "1/1 - 4s - loss: 1.3186 - perplexity: 3.7524 - 4s/epoch - 4s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" nonsense ! it was so nice ! you don \\' t laugh so much . it was right now to say nothing of the big dog ! \" cried the mother , with anglouse that he had done it , and so was the little dog \\' calicoe that the little dog had tied the end of his tail in front of the ring on her , and he was'\n",
            "\n",
            "1/1 - 4s - loss: 1.1851 - perplexity: 3.2829 - 4s/epoch - 4s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" well , don \\' t lose any chance of making trouble for you again . i am so much pleased now that william and laddie and frank and laddie , you have towitaten , i feel , as if to make him lookoutly , \" and he was so surprised to see the matter again . as he did this , i am more often called to ask'\n",
            "\n",
            "1/1 - 4s - loss: 1.0633 - perplexity: 2.9056 - 4s/epoch - 4s/step\n",
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" yes , \" replied the foreman , \" that it was the old - fashioned , but the automobile was not quite so much bigger , as a steamboat was rolling and it needed a steamboat to be seen in the lake . but they did not know it , as far as the shore there was the bunkers , the bunkers , the boards , was'\n",
            "\n",
            "1/1 - 4s - loss: 0.9523 - perplexity: 2.5996 - 4s/epoch - 4s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "b'[BOS] \" i am not afraid to do wrong . but don \\' t care so much for them . it isn \\' t anything of that , if you see , i am working hard myself , i am so much better . but i feel so much better . now i will save my life , and i can spare something for you . i am working hard myself . i \\' d go up to the little'\n",
            "\n",
            "1/1 - 4s - loss: 0.8518 - perplexity: 2.3504 - 4s/epoch - 4s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gKcYvhvuk-fn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}