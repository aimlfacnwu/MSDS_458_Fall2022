{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYLKJ1LZmJZT"
   },
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/NorthwesternHeader.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUwiDnKQmJZb"
   },
   "source": [
    "### Analyze AG_NEWS_SUBSET Data <br>\n",
    "\n",
    "AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity.<br> \n",
    "\n",
    "For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html<br> \n",
    "\n",
    "\n",
    "The AG's news topic classification dataset is constructed by choosing 4 largest classes (**World**, **Sports**, **Business**, and **Sci/Tech**) from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.<br>\n",
    "\n",
    "Homepage: https://arxiv.org/abs/1509.01626<br>\n",
    "\n",
    "Source code: tfds.text.AGNewsSubset\n",
    "\n",
    "Versions:\n",
    "\n",
    "1.0.0 (default): No release notes.\n",
    "Download size: 11.24 MiB\n",
    "\n",
    "Dataset size: 35.79 MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jyg3hsjLmJZd"
   },
   "source": [
    "## References\n",
    "1. Deep Learning with Python, Francois Chollet (https://learning.oreilly.com/library/view/deep-learning-with/9781617296864/)\n",
    " * Chapter 10: Deep learning for time series\n",
    " * Chapter 11: Deep learning for text\n",
    "2. Deep Learning A Visual Approach, Andrew Glassner (https://learning.oreilly.com/library/view/deep-learning/9781098129019/)\n",
    " * Chapter 19: Recurrent Neural Networks\n",
    " * Chapter 20: Attention and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ir9xKmjEmJZe"
   },
   "source": [
    "## The Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfccUEzfmJZe"
   },
   "source": [
    "## Understanding self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2bD03q0mJZf"
   },
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/SelfAttention.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpM0ncXJmJZf"
   },
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/DogAte.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yf-7KKzWmJZg"
   },
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/InputOutput.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2-0mq4EmJZh"
   },
   "source": [
    "## Generalized self-attention: the query-key-value model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Dpn42iOmJZi"
   },
   "source": [
    "#### Retrieving images from a database: the “query” is compared to a set of “keys,” and the match scores are used to rank “values” (images).\n",
    "\n",
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/DogsBeach.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZMx9VAcmJZj"
   },
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AteVsVIpmJZj"
   },
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/MultiHead.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfJGIx4EmJZj"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2988,
     "status": "ok",
     "timestamp": 1648388404506,
     "user": {
      "displayName": "Daniel Jensen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxK0y1Y-W44vjnQC53Ak8_BakmvjpvLobUMJWbWQ=s64",
      "userId": "09835133420012498824"
     },
     "user_tz": 240
    },
    "id": "gnF865oHmJZk"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from packaging import version\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1648388404507,
     "user": {
      "displayName": "Daniel Jensen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxK0y1Y-W44vjnQC53Ak8_BakmvjpvLobUMJWbWQ=s64",
      "userId": "09835133420012498824"
     },
     "user_tz": 240
    },
    "id": "8SO5DzBCmJZm"
   },
   "source": [
    "## Verify TensorFlow Version and Keras Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1648388404508,
     "user": {
      "displayName": "Daniel Jensen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxK0y1Y-W44vjnQC53Ak8_BakmvjpvLobUMJWbWQ=s64",
      "userId": "09835133420012498824"
     },
     "user_tz": 240
    },
    "id": "wMmbJHDImJZm"
   },
   "outputs": [],
   "source": [
    "print(\"This notebook requires TensorFlow 2.0 or above\")\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keras version: \", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive to Colab Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffs4EljMmJZm"
   },
   "source": [
    "## The Transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VSZ6gB_mJZn"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46166,
     "status": "ok",
     "timestamp": 1648388450665,
     "user": {
      "displayName": "Daniel Jensen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxK0y1Y-W44vjnQC53Ak8_BakmvjpvLobUMJWbWQ=s64",
      "userId": "09835133420012498824"
     },
     "user_tz": 240
    },
    "id": "pl0z_01vmJZn",
    "outputId": "5d80d265-59b8-4541-e704-6c58029174f9"
   },
   "outputs": [],
   "source": [
    "# register  ag_news_subset so that tfds.load doesn't generate a checksum (mismatch) error\n",
    "!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=ag_news_subset\n",
    "\n",
    "# Example Approaches to Split Data Set\n",
    "# dataset, info = tfds.load('ag_news_subset', with_info=True,  split=['train[:]','test[:1000]', 'test[1000:]'],\n",
    "dataset, info = tfds.load('ag_news_subset', with_info=True,  split=['train[:95%]','train[95%:]', 'test'],\n",
    "# dataset, info = tfds.load('ag_news_subset', with_info=True,  split=['train[:114000]','train[114000:]', 'test[:]'],\n",
    "                          batch_size = 32, as_supervised=True)\n",
    "train_ds, val_ds, test_ds = dataset\n",
    "# train_dataset, test_dataset = dataset['train'],dataset['test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-3qpx9ZmJZo"
   },
   "source": [
    "## Preparing The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1648388450666,
     "user": {
      "displayName": "Daniel Jensen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxK0y1Y-W44vjnQC53Ak8_BakmvjpvLobUMJWbWQ=s64",
      "userId": "09835133420012498824"
     },
     "user_tz": 240
    },
    "id": "p2_CEJjimJZp"
   },
   "outputs": [],
   "source": [
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytPi4XCImJZp"
   },
   "source": [
    "## Vectorizing The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6712,
     "status": "ok",
     "timestamp": 1648388457370,
     "user": {
      "displayName": "Daniel Jensen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxK0y1Y-W44vjnQC53Ak8_BakmvjpvLobUMJWbWQ=s64",
      "userId": "09835133420012498824"
     },
     "user_tz": 240
    },
    "id": "l-DVBPWGmJZp"
   },
   "outputs": [],
   "source": [
    "max_length = 600\n",
    "max_tokens = 10000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPVABV8vmJZq"
   },
   "source": [
    "## Transformer Encoder Implemented As Subclassed `Layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1648388457371,
     "user": {
      "displayName": "Daniel Jensen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxK0y1Y-W44vjnQC53Ak8_BakmvjpvLobUMJWbWQ=s64",
      "userId": "09835133420012498824"
     },
     "user_tz": 240
    },
    "id": "OscOAvHfmJZq"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtiaDM_nmJZq"
   },
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/TransformerEncoder.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQNlcHvpmJZr"
   },
   "source": [
    "## Using Transformer Encoder For Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1648388457640,
     "user": {
      "displayName": "Daniel Jensen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxK0y1Y-W44vjnQC53Ak8_BakmvjpvLobUMJWbWQ=s64",
      "userId": "09835133420012498824"
     },
     "user_tz": 240
    },
    "id": "KNKHwoBAmJZr",
    "outputId": "8f41fce2-54c4-45ce-ad1d-b4edfb99a234"
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"SparseCategoricalCrossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjpJFn52mJZr"
   },
   "source": [
    "## Training and Evaluating Transformer Encoder Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2324341,
     "status": "ok",
     "timestamp": 1648390781978,
     "user": {
      "displayName": "Daniel Jensen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxK0y1Y-W44vjnQC53Ak8_BakmvjpvLobUMJWbWQ=s64",
      "userId": "09835133420012498824"
     },
     "user_tz": 240
    },
    "id": "oJP3RBaBmJZr",
    "outputId": "132160fe-57f2-418c-96c1-2710cde47e78"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49CU5NhjmJZs"
   },
   "source": [
    "## Using Positional Encoding to Re-Inject Order Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6thjuXg1mJZs"
   },
   "source": [
    "## Implementing Positional Embedding As Subclassed Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1648390781979,
     "user": {
      "displayName": "Daniel Jensen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxK0y1Y-W44vjnQC53Ak8_BakmvjpvLobUMJWbWQ=s64",
      "userId": "09835133420012498824"
     },
     "user_tz": 240
    },
    "id": "wZ5-2z1WmJZs"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwekVVu5mJZt"
   },
   "source": [
    "## Putting it all together: A text-classification Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "friLWNBzmJZt"
   },
   "source": [
    "## Combining Transformer Encoder with Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4602975,
     "status": "ok",
     "timestamp": 1648395384944,
     "user": {
      "displayName": "Daniel Jensen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxK0y1Y-W44vjnQC53Ak8_BakmvjpvLobUMJWbWQ=s64",
      "userId": "09835133420012498824"
     },
     "user_tz": 240
    },
    "id": "IeVw84ygmJZt",
    "outputId": "770962ae-34b3-4c39-95d4-cb53fd37d2dc"
   },
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lyfC1axmJZu"
   },
   "source": [
    "## When To Use Sequence Models over Bag-of-Words Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lh30ecbrmJZu"
   },
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/SequenceVBagWords.png?raw=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1648395384945,
     "user": {
      "displayName": "Daniel Jensen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxK0y1Y-W44vjnQC53Ak8_BakmvjpvLobUMJWbWQ=s64",
      "userId": "09835133420012498824"
     },
     "user_tz": 240
    },
    "id": "1lqhPc3ImJZu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MSDS458_Assignment_03_2022_EA_part03_Transformer_20220325_DEV_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
