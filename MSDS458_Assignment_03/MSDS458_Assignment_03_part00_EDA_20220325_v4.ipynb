{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cfh9kVB-rpjo"
   },
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/NorthwesternHeader.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRAx1f1ir1St"
   },
   "source": [
    "## MSDS458 Research Assignment 3 - Part 00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQnyR1_aFk-t"
   },
   "source": [
    "## Analyze AG_NEWS_SUBSET Data <br>\n",
    "\n",
    "AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity.<br> \n",
    "\n",
    "For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html<br> \n",
    "\n",
    "\n",
    "The AG's news topic classification dataset is constructed by choosing 4 largest classes (**World**, **Sports**, **Business**, and **Sci/Tech**) from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.<br>\n",
    "\n",
    "Homepage: https://arxiv.org/abs/1509.01626<br>\n",
    "\n",
    "Source code: tfds.text.AGNewsSubset\n",
    "\n",
    "Versions:\n",
    "\n",
    "1.0.0 (default): No release notes.\n",
    "Download size: 11.24 MiB\n",
    "\n",
    "Dataset size: 35.79 MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGBlk-2-NaCn"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>More Technical</b>: Throughout the notebook. This types of boxes provide more technical details and extra references about what you are seeing. They contain helpful tips, but you can safely skip them the first time you run through the code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda tensorflow-datasets --y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Deep learning for text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Natural-language processing: The bird's eye view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handcrafted rules held out as the dominant approach well into the 1990s. But starting in the late 1980s, faster computers and greater data availability started making a better alternative viable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started seeing machine learning approaches to natural language processing. The earliest ones were based on `decision trees`—the intent was literally to automate the development of the kind of if/then/else rules of previous systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then statistical approaches started gaining speed, starting with `logistic regression`. Over time, learned parametric models fully took over, and linguistics came to be seen as more of a hindrance than a useful tool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In early 2015, Keras made available the first open source, easy-to-use implementation of `LSTM`, just at the start of a massive wave of renewed interest in recurrent neural networks—until then, there had only been “research code” that couldn’t be readily reused. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then from 2015 to 2017, recurrent neural networks dominated the booming NLP scene. `Bidirectional LSTM` models, in particular, set the state of the art on many important tasks, from summarization to question-answering to machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, around 2017–2018, a new architecture rose to replace RNNs: the `Transformer`. Transformers unlocked considerable progress across the field in a short period of time, and today most NLP systems are based on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Natural Language Processing Tasks\n",
    "\n",
    "The applications of natural language algorithms are commonly called tasks. Here are some popular tasks:\n",
    "\n",
    "1. **Sentiment Analysis**: Given opinionated text like a movie review, determine whether the overall sense is positive or negative.\n",
    "2. **Translation**: Turn text into another language.\n",
    "Answer Questions: Answer questions about the text, like who is the hero, or what actions occurred.\n",
    "3. **Summarize or Paraphrase**: Provide a short overview of the text, emphasizing the main points.\n",
    "4. **Generate New Text**: Given some starting text, write more text that seems to follow from it.\n",
    "5. **Logical Flow**: If a sentence first asserts a premise and the following sentence asserts a conclusion based on that premise, determine whether the conclusion logically follows from the premise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Preparing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/11-01.png?raw=1\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Text standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text standardization is a basic form of feature engineering that aims to erase encoding differences that you don’t want your model to have to deal with. It’s not exclusive to machine learning, either—you’d have to do the same thing if you were building a search engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest and most widespread standardization schemes is “convert to lowercase and remove punctuation characters.” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these standardization techniques, your model will require less training data and will generalize better—it won’t need abundant examples of both “Sunset” and “sunset” to learn that they mean the same thing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Text splitting (tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your text is standardized, you need to break it up into units to be vectorized (tokens), a step called tokenization. You could do this in three different ways:\n",
    "\n",
    "* `Word-level tokenization`—Where tokens are space-separated (or punctuation-separated) substrings. A variant of this is to further split words into subwords when applicable—for instance, treating “staring” as “star+ing” or “called” as “call+ed.”\n",
    "* `N-gram tokenization`—Where tokens are groups of N consecutive words. For instance, “the cat” or “he was” would be 2-gram tokens (also called bigrams).\n",
    "* `Character-level tokenization`—Where each character is its own token. In practice, this scheme is rarely used, and you only really see it in specialized contexts, like text generation or speech recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a simple example. Consider the sentence “the cat sat on the mat.” It may be decomposed into the following set of 2-grams:\n",
    "\n",
    "`{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n",
    " \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may also be decomposed into the following set of 3-grams:\n",
    "\n",
    "`{\"the\", \"the cat\", \"cat\", \"cat sat\", \"the cat sat\",\n",
    " \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\",\n",
    " \"sat on the\", \"the mat\", \"mat\", \"on the mat\"}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`One-dimensional convnets`, `recurrent neural networks`, and `Transformers` are capable of learning representations for groups of words and characters without being explicitly told about the existence of such groups, by looking at continuous word or character sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Vocabulary indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build an index of all terms found in the training data (the “vocabulary”), and assign a unique integer to each entry in the vocabulary.\n",
    "```python\n",
    "vocabulary = {} \n",
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. You can then convert that integer into a vector encoding that can be processed by a neural network, like a one-hot vector:\n",
    "```python\n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1 \n",
    "    return vector\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Using the TextVectorization layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import string\n",
    "  \n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text \n",
    "                       if char not in string.punctuation)\n",
    "  \n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "  \n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "  \n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "  \n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "  \n",
    "vectorizer = Vectorizer()\n",
    "# Haiku by poet Hokushi\n",
    "dataset = [           \n",
    "    \"I write, erase, rewrite\",   \n",
    "    \"Erase again, and then\",     \n",
    "    \"A poppy blooms.\",           \n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> test_sentence = \"I write, rewrite, and still rewrite again\" \n",
    ">>> encoded_sentence = vectorizer.encode(test_sentence)\n",
    ">>> print(encoded_sentence)\n",
    "[2, 3, 5, 7, 1, 5, 6]\n",
    ">>> decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    ">>> print(decoded_sentence)\n",
    "\"i write rewrite and [UNK] rewrite again\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, using something like this wouldn’t be very performant. In practice, you’ll work with the Keras `TextVectorization` layer, which is fast and efficient and can be dropped directly into a `tf.data` pipeline or a Keras model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",    # Configures the layer to return sequences of words encoded as integer indices. \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `TextVectorization` layer will use the setting “convert to lowercase and remove punctuation” for text standardization, and “split on whitespace” for tokenization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import re \n",
    "import string \n",
    "import tensorflow as tf\n",
    "  \n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)  # Convert strings to lowercase.\n",
    "    return tf.strings.regex_replace(                 # Replace punctuation characters with the empty string.\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "  \n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)      # Split strings on whitespace.\n",
    " \n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    spl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import re \n",
    "import string \n",
    "import tensorflow as tf\n",
    "  \n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)              ❶\n",
    "    return tf.strings.regex_replace(                                ❷\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "  \n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)                          ❸\n",
    " \n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❶ Convert strings to lowercase.\n",
    "\n",
    "❷ Replace punctuation characters with the empty string.\n",
    "\n",
    "❸ Split strings on whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To index the vocabulary of a text corpus, just call the `adapt()` method of the layer with a `Dataset` object that yields strings, or just with a list of Python strings:\n",
    "```python\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "text_vectorization.adapt(dataset)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can retrieve the computed vocabulary via `get_vocabulary()`\n",
    "```python\n",
    ">>> text_vectorization.get_vocabulary()\n",
    "[\"\", \"[UNK]\", \"erase\", \"write\", ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method can be useful if you need to convert text encoded as integer sequences back into words. The first two entries in the vocabulary are the `mask` token (index 0) and the `OOV` token (index 1). \n",
    "\n",
    "Entries in the vocabulary list are sorted by frequency, so with a real-world dataset, very common words like “the” or “a” would come first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two ways we could use our `TextVectorization` layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  The first option is to put it in the `tf.data pipeline`, like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways we could use our TextVectorization layer. The first option is to put it in the `tf.data pipeline`, like this:\n",
    "```python\n",
    "int_sequence_dataset = string_dataset.map(   ❶\n",
    "    text_vectorization,\n",
    "    num_parallel_calls=4)                    ❷\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❶ string_dataset would be a dataset that yields string tensors.\n",
    "\n",
    "❷ The num_parallel_calls argument is used to parallelize the map() call across multiple CPU cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The second option is to make it part of the `model` (after all, it’s a Keras layer), like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second option is to make it part of the model (after all, it’s a Keras layer), like this:\n",
    "```python\n",
    "text_input = keras.Input(shape=(), dtype=\"string\")             ❶\n",
    "vectorized_text = text_vectorization(text_input)               ❷\n",
    "embedded_input = keras.layers.Embedding(...)(vectorized_text)  ❸\n",
    "output = ...                                                   ❸\n",
    "model = keras.Model(text_input, output) \n",
    "```\n",
    "❶ Create a symbolic input that expects strings.\n",
    "\n",
    "❷ Apply the text vectorization layer to it.\n",
    "\n",
    "❸ You can keep chaining new layers on top—just your regular Functional API model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***There’s an important difference between the two**: if the vectorization step is part of the model, it will happen synchronously with the rest of the model. This means that at each training step, the rest of the model (placed on the GPU) will have to wait for the output of the TextVectorization layer (placed on the CPU) to be ready in order to get to work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, putting the layer in the `tf.data` pipeline enables you to do asynchronous preprocessing of your data on CPU: while the GPU runs the model on one batch of vectorized data, the CPU stays busy by vectorizing the next batch of raw strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if you’re training the model on GPU or TPU, you’ll probably want to go with the first option to get the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you were to export our model to a production environment, you would want to ship a model that accepts raw strings as input—otherwise you would have to reimplement text standardization and tokenization in your production environment (maybe in JavaScript?), and you would face the risk of introducing small preprocessing discrepancies that would hurt the model’s accuracy. Thankfully, the TextVectorization layer enables you to include text preprocessing right into your model, making it easier to deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting a model that processes raw strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")   ❶\n",
    "processed_inputs = text_vectorization(inputs)      ❷\n",
    "outputs = model(processed_inputs)                  ❸\n",
    "inference_model = keras.Model(inputs, outputs)     ❹\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❶ One input sample would be one string.\n",
    "\n",
    "❷ Apply text preprocessing.\n",
    "\n",
    "❸ Apply the previously trained model.\n",
    "\n",
    "❹ Instantiate the end-to-end model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model can process batches of `raw strings`:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I loved it.\"],\n",
    "])\n",
    "predictions = inference_model(raw_text_data) \n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-Jjr3rNr5PR"
   },
   "source": [
    "## Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbrX9qUZvfs1"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from packaging import version\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0H8HuzHrWwN"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKucNfkrsqrP"
   },
   "source": [
    "## Verify TensorFlow Version and Keras Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_BoGg9JrWzj",
    "outputId": "2370d763-86ee-4648-8d37-d631853ecf52"
   },
   "outputs": [],
   "source": [
    "print(\"This notebook requires TensorFlow 2.0 or above\")\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_bLWHP4rW3b",
    "outputId": "0c893013-01f5-44fe-fcb9-598cef6b8449"
   },
   "outputs": [],
   "source": [
    "print(\"Keras version: \", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_validation_report(test_labels, predictions):\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(test_labels, predictions))\n",
    "    print('Accuracy Score: {}'.format(accuracy_score(test_labels, predictions)))\n",
    "    print('Root Mean Square Error: {}'.format(np.sqrt(MSE(test_labels, predictions)))) \n",
    "    \n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.75,  cbar=False, ax=ax,cmap='Blues',linecolor='white')\n",
    "    #  square=True,\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mFyXjJmtJ3E"
   },
   "source": [
    "## Mount Google Drive to Colab Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXm3H81YrW8-"
   },
   "outputs": [],
   "source": [
    "##from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AG_NEWS_SUBSET Data in CSV files \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b> ag_news_subset</b><br>\n",
    "    See  https://www.kaggle.com/amananandrai/ag-news-classification-dataset\n",
    " \n",
    "  <li>We combined the Title and Description columns into a Title_Description column in the training and test csv files.</li>\n",
    "  <li>We also created a validation csv file using some of the data from the training csv file.</li>\n",
    "</ol>\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "val_df = pd.read_csv('./data/val.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "print(f\"There are {train_df.shape[0]} news articles for training.\")\n",
    "print(f\"There are {val_df.shape[0]} news articles for validation.\")\n",
    "print(f\"There are {test_df.shape[0]} news articles for testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv('./data/all.csv')\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n",
    "all_df.Label.value_counts(sort=False).rename(index=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a dataset pipeline to use for our model specifying the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds =\\\n",
    "tf.data.experimental.make_csv_dataset('./data/train.csv',select_columns=['Title_Description','Label'],\n",
    "                                      label_name=\"Label\",num_epochs=1, batch_size=batch_size)\n",
    "val_ds =\\\n",
    "tf.data.experimental.make_csv_dataset('./data/val.csv',select_columns=['Title_Description','Label'],\n",
    "                                      label_name=\"Label\",num_epochs=1,batch_size=batch_size)\n",
    "test_ds =\\\n",
    "tf.data.experimental.make_csv_dataset('./data/test.csv',select_columns=['Title_Description','Label'],\n",
    "                                      label_name=\"Label\",num_epochs=1,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(f\"The first batch has {inputs['Title_Description'].shape[0]} articles with {targets.shape[0]} labels.\")\n",
    "    print(f\"The articles have type {inputs['Title_Description'].dtype}, the labels type: {targets.dtype}\\n\")\n",
    "    \n",
    "    # just print the first news article in the batch\n",
    "    print(f\"The first news article:\\n{inputs['Title_Description'][0]}\\n\") \n",
    "\n",
    "    print(f\"It is a {categories[targets[0].numpy()]} article.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data set of all news articles for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds = tf.data.experimental.make_csv_dataset('./data/all.csv',select_columns=['Title_Description','Label'],\n",
    "                                      label_name=\"Label\",num_epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    ")\n",
    "text_only_all_ds = all_ds.map(lambda x, y: x['Title_Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in text_only_all_ds:\n",
    "    print(f\"Get first batch of {text.shape[0]} news articles.\\n\")\n",
    "    print(f\"Here is the first news article:\\n\\n{text[0]}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_vectorization.adapt(text_only_all_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = text_vectorization.get_vocabulary()\n",
    "print(f\"There are {len(vocab)} vocabulary words in the corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "doc_sizes = []\n",
    "corpus = []\n",
    "for example, _ in all_ds:\n",
    "  enc_example = text_vectorization(example['Title_Description'])\n",
    "  doc_sizes.append(enc_example.shape[1])\n",
    "  corpus+=list(enc_example.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(corpus)} words in the corpus of {len(doc_sizes)} news articles.\")\n",
    "print(f\"Each news article has between {min(doc_sizes)} and {max(doc_sizes)} tokens in it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "plt.hist(doc_sizes, bins=20,range = (0,120))\n",
    "plt.xlabel(\"Tokens Per Document\")\n",
    "plt.ylabel(\"Number of AG News Articles\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "encoder_1000 = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    max_tokens=1000\n",
    ")\n",
    "encoder_1000.adapt(text_only_all_ds)\n",
    "vocab_1000 = np.array(encoder_1000.get_vocabulary());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_1000[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"the dog ran after a red ball as it rolled by the hat on the ground.\"\n",
    "enc_example = encoder_1000(example).numpy()\n",
    "enc_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ones = tf.math.count_nonzero(enc_example==1).numpy()\n",
    "num_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "doc1000_sizes = []\n",
    "corpus1000 = []\n",
    "count1000=0\n",
    "useless = 0\n",
    "# stop = 0\n",
    "percents = []\n",
    "for example, _ in all_ds.as_numpy_iterator():\n",
    "#   stop+=1\n",
    "#   if stop > 2: break\n",
    "  enc_example = encoder_1000(example['Title_Description'])[0]\n",
    "#   print(enc_example)\n",
    "\n",
    "  num_ones = tf.math.count_nonzero(enc_example==1).numpy()\n",
    "#   print(num_ones)\n",
    "  percent_ones = round(num_ones*100/len(enc_example))\n",
    "#   print(f\"{percent_ones}%\")\n",
    "  percents.append(percent_ones)\n",
    "\n",
    "  s = set(list(enc_example.numpy()))\n",
    "  if s == {1}: useless+=1\n",
    "#   print(useless)\n",
    "\n",
    "  doc1000_sizes.append(len(enc_example))\n",
    "  corpus1000+=list(enc_example.numpy())\n",
    "\n",
    "  count1000 += tf.math.count_nonzero(enc_example>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "plt.hist(percents, 20)\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.xlabel('Percent of Non-Vocabulary Words in a Document');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Each news article has between {min(doc1000_sizes)} and {max(doc1000_sizes)} tokens in it.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MSDS458_Assignment_03_20210619_DEV_v16.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
